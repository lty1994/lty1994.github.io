---
title: 决策树基础
date: 2019-04-15 11:04:25
tags:
- 监督学习
- 算法模型
categories:
- 学习
- 机器学习
---

### 决策树（Decision Tree）

决策树是一种基本的分类和回归的方法，它是一种树形结构。其每个非叶节点表示一个**特征属性**上的测试，每个分支代表这个特征属性在其取值范围内的某一输出，而每个叶节点存放一个**类别**。

决策树进行**决策的过程**就是从根节点开始，测试待分类项中相应的特征属性，并按照其值选择输出分支，直到到达叶子节点，将叶子节点存放的类别作为决策结果。

决策树学习本质上是从训练数据集中归纳出一组分类规则。通常用损失函数表示这一目标，损失函数通常是正则化的极大似然函数，**学习的策略是以损失函数为目标函数的最小化。**学习的算法一般是启发式方法，近似求解最优值。学习算法是一个递归选择最优特征的过程。为了避免最后学习到的决策树发生过拟合现象，我们通常使用验证集来对学习到的决策树进行剪枝，使其有更好的泛化能力。

![决策树结构](https://o9wdag.ch.files.1drv.com/y4mOs4BhnCR_drkQjLvsWHyugFVX8gfiPGrCOVK2PuQUD8FH9dmzbiEL3JYB4Z0GtJfTEte5u_UDkWaXC5gsV58z_NCdSWe5tSh8GljXMqqHLYNCU-TxUTuMJ02fZDhWIdDlBAn87AKmD4cU3SZfm5XdjetIDVSm-o0wNzvFNVL5UjK72EHrvZ2sWlOO4aDp_LSh6c5QYiG9MNm5RbbodT9fg?width=706&height=462&cropmode=none)

**决策树的结构有以下特点：**

- 结点和有向边组成
- 结点有内部结点和叶结点俩种类型
- 内部结点表示一个特征，叶节点表示一个类，边表示该特征下可能的输出值

**决策树学习的步骤通常包括：**

- 特征选择
- 决策树的生成
- 决策树的剪枝

下面从决策树学习的三个步骤介绍决策树模型的原理：

### 特征选择

特征选择的目的在于选取对训练数据能够分类的特征，常用的特征选择算法有ID.3、C4.5和基尼系数，相关的公式如下：

#### ID.3

样本集合D对特征A的信息增益
$$
g(D,A)=H(D)-H(D|A)\\
H(D)=-\sum_{k=1}^{K}\frac{\left | C_{k} \right |}{\left | D \right |}\log_{2}\frac{\left | C_{k} \right |}{\left | D \right |}\\
H(D|A)=-\sum_{i=1}^{n}\frac{\left | D_{i} \right |}{\left | D \right |}H(D_{i})
$$
其中$H(D)$是数据集D的熵，$H(D_{i})$是数据集$D_{i}$的熵，$H(D|A)$是数据集D对特征A的条件熵。$D_{i}$是D中特征A取第i个值的样本子集，$C_{k}$是D中属于第k类的样本子集。n是特征A取值的个数，K是类的个数。

#### C4.5

$$
g_{R}(D,A)=\frac{g(D,A)}{H_{A}(D)}
$$

其中，$g(D,A)$是信息增益，$H_{A}(D)$是D关于特征A的值的熵。

#### CART（分类问题）

$$
Gini(D)=1-\sum_{k=1}^{K}\left ( \frac{\left | C_{k} \right |}{\left | D \right |} \right )^2
$$

特征A条件下集合D的基尼系数：
$$
Gini(D,A)=\frac{\left | D_{1} \right |}{\left | D \right |}Gini(D_{1})+\frac{\left | D_{2} \right |}{\left | D \right |}Gini(D_{2})
$$

### 决策树的生成

通常使用上述特征选择的方法，从根节点开始，递归的生成决策树。这里主要讲一下CART回归树的生成，CART对**回归树用平方误差最小化准则，对分类树用基尼系数最小化准则**，进行特征选择，生成树。

#### 回归树

假设X,Y为输入和输出，Y为连续变量。假设我们将输入空间划分为M个单元$R_{1}...R_{M}$，并且每个单元$R_{m}$上有一个固定的输出值$c_{m}$，模型可以表示为：
$$
f(x)=\sum_{m=1}^{M}c_{m}I(x\in{R_{m}})
$$
我们可以用平方误差$\sum_{x_{i}\in{R_{m}}}(y_{i}-f(x_{i}))^2​$来表示回归树的训练误差，易知单元$R_m​$上的$c_m​$的最优值是单元上所有输入实例对应的输出值的均值：
$$
\hat c_m=ave(y_i|x_i \in{R_m})
$$
对于空间的划分，我们选择第j个特征$x^{(j)}$和它的取值s，并定义两个区域：
$$
R_1(j,s)=\{x|x^{(j)}\leq s\}\hspace{3em}\hat R_2(j,s)=\{x|x^{(j)}> s\}
$$
然后求解：
$$
\min_{j,s}\left [ \min_{c_1}\sum_{x_{i}\in{R_{1}(j,s)}}(y_{i}-c_1)^2 +\min_{c_2}\sum_{x_{i}\in{R_{2}(j,s)}}(y_{i}-c_2)^2 \right ]
$$
对于固定的输入变量j，可以找到s.
$$
\hat c_m=\frac{1}{N_m}\sum_{x_i \in{R_{m}(j,s)}}y_i\hspace{1em}(x\in R_m,m=1,2)
$$
然后遍历j，找到最优的（j，s）对。然后依据此对空间做一次划分，重复上述过程。这样生成的树我们通常称为最小二乘回归树。**每一次划分后，下一次拟合的是残差。**

### 决策树的剪枝

决策树的剪枝**通过极小化决策树整体的Loss来实现**，假设决策树T的叶节点个数为|T|，t是树T的叶节点，该叶节点有$N_{t}$个样本点，其中k类的样本点有$N_{tk}$个，$k=1,2,...,K$，$H_{t}(T)$为叶节点t上的经验熵，$\alpha \geq 0$为参数，则决策树学习的损失函数定义为：(其中经验熵为：$H_{t}(T)=-\sum_{k=1}^{K}\frac{N_{tk}}{N_{t}}\log \frac{N_{tk}}{N_{t}}$)
$$
\begin{align}
C_{\alpha}(T)&=\sum_{t=1}^{|T|}N_{t}H_{t}(T)+\alpha|T|\\
&=-\sum_{t=1}^{|T|}\sum_{k=1}^{K}N_{tk}\log \frac{N_{tk}}{N_{t}}+\alpha|T|\\
&=C(T)+\alpha|T|
\end{align}
$$
上式中C(T)表示模型对训练数据的预测误差，即模型和训练数据的拟合程度，|T|表示模型的复杂度。剪枝就是当α确定时，选择损失函数最小的模型，即损失函数最小的子树。

> 算法：
>
> 输入：生成树T，参数α
>
> 输出：修剪后的子树$T_{\alpha}​$
>
> 1. 计算每个节点的经验熵
> 2. 递归的从树的叶节点向上回缩，**如果回缩到父节点前后的损失函数值不增加，那么进行剪枝，将父节点变为新的叶节点**
> 3. 返回2，直至不能继续，得到损失函数最小的子树$T_{\alpha}$